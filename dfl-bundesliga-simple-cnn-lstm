 So in this notebook I will try to create some model, which takes both factors into account: image and 
 seqaunce analysis. That's why I came up with the Idea of the CNN Model for frames, which afterwards is 
 fed into the LSTM. To achive this I split the data and feed every second (so 25 frames) into the mode. I assign the
 closest (by time) event to every second. Each input in the end is of the shape (25,3,128,128), with an output of 5 classes 
 (challenge, throwin, play, start, end). In the end, i will use only challenge, throwin and play classes for the prediction. 
 The Inference of this model can be found here. I use really cool resized dataset of yokuyama in this notebook.
 
 Import libraries
 
 from google.colab import drive
drive.mount('/content/drive')


import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import cv2
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from prettytable import PrettyTable
from sklearn.model_selection import train_test_split
for dirname, _, filenames in os.walk('drive/MyDrive/DFL-Bundesliga-Data-Shootout1/train_resize'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        
        
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)


def count_parameters(model):
    table = PrettyTable(["Modules", "Parameters"])
    total_params = 0
    for name, parameter in model.named_parameters():
        if not parameter.requires_grad: continue
        params = parameter.numel()
        table.add_row([name, params])
        total_params+=params
    print(table)
    print(f"Total Trainable Params: {total_params}")
    return total_params
    
    
df_train = pd.read_csv("drive/MyDrive/DFL-Bundesliga-Data-Shootout1/train.csv")
df_train["file_path"] = "drive/MyDrive/DFL-Bundesliga-Data-Shootout1/train_resize/" + df_train["video_id"] + ".mp4"
df_train.head(20)


Here I create the dataframe where every video is split in a second by second patch
video_names = df_train.video_id.unique()
video_names


labels_time = []
video_ids = []
for name in video_names:    
    time_to_end = df_train[df_train["video_id"]==name].iloc[-1].time
    TIME_SPLIT = 1 # 25 frames
    times_of_one_video = np.arange(0,time_to_end,TIME_SPLIT)
    labels_time.extend(times_of_one_video)
    video_ids.extend([name]*len(times_of_one_video))
D = {"video_id":video_ids, "time":labels_time}
df_labels = pd.DataFrame(D)
display(df_labels)


labels = []
paths = []
for i, row in tqdm(df_labels.iterrows()):
    df_train_only_specific_video = df_train[df_train.video_id == row.video_id]
    video_clip = df_train_only_specific_video.iloc[(df_train_only_specific_video['time'] - row.time).abs().argsort()[:1]]
    video_clip = video_clip.iloc[0]
    labels.append(video_clip.event)
    paths.append(video_clip.file_path)
df_labels["event"] = labels
df_labels["file_path"] = paths
df_labels


num_start = df_labels[df_labels.event == "start"].event.count()
num_end = df_labels[df_labels.event == "end"].event.count()
num_throwin = df_labels[df_labels.event == "throwin"].event.count()
num_challenge = df_labels[df_labels.event == "challenge"].event.count()
num_play = df_labels[df_labels.event == "play"].event.count()
sns.histplot(df_labels, x = "event")          


Dataset
class VideoFrameDataset(Dataset):
    def __init__(self, df):
        self.df = df
        self.frames_split = 25 * TIME_SPLIT # 25 frames per second
        
    def __len__(self):
        return self.df.shape[0]
    
    def __getitem__(self, index):
        event = self.df.iloc[index].event
        file_path = self.df.iloc[index].file_path
        time = self.df.iloc[index].time
        D = {"challenge":0, "throwin":1, "play":2, "start":3, "end":4}
        label = D[event]
        cap = cv2.VideoCapture(file_path)
        video_clip = np.zeros((self.frames_split, 128, 128, 3))
        #skip all unused frames
        cap.set(cv2.CAP_PROP_POS_FRAMES,time*self.frames_split)
        for i in range(self.frames_split):
            _, frame = cap.read()
            frame = cv2.resize(frame, (128,128))
            video_clip[i] = frame
        video_clip = video_clip.transpose(0, 3, 1, 2)
        return torch.from_numpy(video_clip), label
        
        
        
model
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 10, 5)
        self.conv2 = nn.Conv2d(10, 20, 5)
        self.conv3 = nn.Conv2d(20, 30, 5)
        
    def forward(self, i):
        x = i.view(-1, i.shape[2], i.shape[3], i.shape[4])
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = nn.AvgPool2d(4)(x)
        x = x.view(i.shape[0], i.shape[1], -1)
        return x
    
class LSTM(nn.Module):
    def __init__(self):
        super(LSTM, self).__init__()
        self.lstm = nn.LSTM(25230, 100)
        self.fc = nn.Linear(2500, 5)
        
    def forward(self, x):
        x, _ = self.lstm(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)
        return x    

class LSTM_and_CNN(nn.Module):
    def __init__(self):
        super(LSTM_and_CNN, self).__init__()
        self.model_cnn = CNN().to(device)
        self.model_lstm = LSTM().to(device)
    
    def forward(self, x):
        features = self.model_cnn(x)
        out = self.model_lstm(features)
        return out
        
        
        
 model = LSTM_and_CNN().to(device)
count_parameters(model)


Prepare before training
Here I load the dataset into the DataLoader and take into account, that the data is unbalanced, so I create weights for every output event, according to their occurances in the dataset.

train, val = train_test_split(df_labels, test_size=0.1, random_state=42, stratify = df_labels.event)
batch_size = 1
train_loader = DataLoader(
    VideoFrameDataset(train), 
    batch_size=batch_size, 
    shuffle=False, 
    num_workers=1
)
val_loader = DataLoader(
    VideoFrameDataset(val), 
    batch_size=batch_size, 
    shuffle=False, 
    num_workers=1
)
dataloaders_dict = {"train": train_loader, "val": val_loader}
weights = torch.zeros(5)
weights[0] = 1/num_challenge
weights[1] = 1/num_throwin
weights[2] = 1/num_play
weights[3] = 1/num_start
weights[4] = 1/num_end
criterion = nn.CrossEntropyLoss(weight = weights, reduction = 'mean')



Train
def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, train_loss_list = [], val_loss_list = [], train_acc_list = [], val_acc_list = []):
    best_acc = 0.0
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    for epoch in range(num_epochs):
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()
                
            epoch_loss = 0.0
            epoch_acc = 0
            
            dataloader = dataloaders_dict[phase]
            for item in tqdm(dataloader, leave=False):
                frames = item[0].to(device).float()
                classes = item[1].to(device).long()

                optimizer.zero_grad()
                
                with torch.set_grad_enabled(phase == 'train'):
                    output = model(frames)
                    loss = criterion(output, classes)
                    _, preds = torch.max(output, 1)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                    epoch_loss += loss.item() * len(output)
                    epoch_acc += torch.sum(preds == classes.data)
                    

            data_size = len(dataloader.dataset)
            epoch_loss = epoch_loss / data_size
            epoch_acc = epoch_acc.double() / data_size
            if phase == "train":
                train_loss_list.append(epoch_loss)
                train_acc_list.append(epoch_acc.cpu())
            else:
                val_loss_list.append(epoch_loss)
                val_acc_list.append(epoch_acc.cpu())
            print(f'Epoch {epoch + 1}/{num_epochs} | {phase:^5} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}')
        
        if epoch_acc > best_acc:
            torch.save(model.state_dict(), 'lstm_cnn.pth')
            best_acc = epoch_acc
    return train_loss_list, val_loss_list, train_acc_list, val_acc_list
num_epochs = 4
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
weight = criterion.weight.to(device)
criterion = nn.CrossEntropyLoss(weight=weight)
train_loss_list, val_loss_list, train_acc_list, val_acc_list = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs)





